{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66b5be4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T05:31:39.972983400Z",
     "start_time": "2023-12-05T05:31:31.784223400Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, classification_report, roc_curve, auc, confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from transformers import BertTokenizer, BertForMaskedLM, pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import random\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c205812",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T05:31:40.700648600Z",
     "start_time": "2023-12-05T05:31:39.976985600Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m.dataset.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m data\u001B[38;5;241m.\u001B[39mdrop_duplicates(subset\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muu_usl_name\u001B[39m\u001B[38;5;124m'\u001B[39m, keep\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfirst\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      3\u001B[0m                                                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      4\u001B[0m data\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSERVICE_CLASS_CONFIRMED\u001B[39m\u001B[38;5;124m'\u001B[39m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\NLP\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m    935\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    936\u001B[0m     dialect,\n\u001B[0;32m    937\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    944\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m    945\u001B[0m )\n\u001B[0;32m    946\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 948\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\NLP\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    608\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    610\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 611\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    613\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    614\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\PycharmProjects\\NLP\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1445\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1447\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1448\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\NLP\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1703\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1704\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1705\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1706\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1707\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1708\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1709\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1710\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1711\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1712\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1713\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1714\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1715\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1716\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\PycharmProjects\\NLP\\venv\\lib\\site-packages\\pandas\\io\\common.py:863\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    858\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    859\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    860\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    861\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    862\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 863\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    864\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    865\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    866\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    867\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    868\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    869\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    870\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    871\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    872\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '.dataset.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('.dataset.csv')\n",
    "data.drop_duplicates(subset='uu_usl_name', keep='first'\n",
    "                                                '', inplace=True)\n",
    "data.drop('SERVICE_CLASS_CONFIRMED', axis=1, inplace=True)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5f00a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T17:02:48.991635100Z",
     "start_time": "2023-12-04T17:02:48.984095300Z"
    }
   },
   "outputs": [],
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e423d69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T05:33:38.467644200Z",
     "start_time": "2023-12-05T05:31:42.788025500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "73bc48415d0d4e7e863c2a2d52bf3f07"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\PycharmProjects\\NLP\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "source.spm:   0%|          | 0.00/1.08M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab43ae95ccc44fb188474f966f2b3b77"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "target.spm:   0%|          | 0.00/803k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "25e6eefa1fa0435e8fd775bcad96a2c2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/2.60M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2b5833e42fde41cfae2fb11c05651b4a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4fd505cc1d814e26a769da8d5796ef00"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\PycharmProjects\\NLP\\venv\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "pytorch_model.bin:   0%|          | 0.00/307M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3708b4a3df5d46888b1e9129a97bc03f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dbcc8552add04863bfeaf8ca974d168f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a96bd752c6d49fcb95dd98df715aba9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "source.spm:   0%|          | 0.00/803k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9397780596f94da389eab80322bbc274"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "target.spm:   0%|          | 0.00/1.08M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "550529e7fac2437593dcc0030cbcd03d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/2.60M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "58e7d9756e5f4ecd9b318f4ea53c03ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1da444e4064420ea6b19a0a829497b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "pytorch_model.bin:   0%|          | 0.00/307M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf4483d6f69c43bf80a2053f314ee86b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e86254128224cf783f260d290b1be4f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model_name_ru_to_en = 'Helsinki-NLP/opus-mt-ru-en'\n",
    "model_name_en_to_ru = 'Helsinki-NLP/opus-mt-en-ru'\n",
    "\n",
    "tokenizer_ru_to_en = MarianTokenizer.from_pretrained(model_name_ru_to_en)\n",
    "model_ru_to_en = MarianMTModel.from_pretrained(model_name_ru_to_en)\n",
    "\n",
    "tokenizer_en_to_ru = MarianTokenizer.from_pretrained(model_name_en_to_ru)\n",
    "model_en_to_ru = MarianMTModel.from_pretrained(model_name_en_to_ru)\n",
    "\n",
    "# Функция для перевода текста\n",
    "def translate(text, model, tokenizer, target_language=\"en\"):\n",
    "    tokenized_text = tokenizer.prepare_seq2seq_batch([text], return_tensors='pt')\n",
    "    translated = model.generate(**tokenized_text)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Функция для back-translation\n",
    "def back_translate(text, source_language=\"ru\", target_language=\"en\"):\n",
    "    # Переводим с русского на английский\n",
    "    translated_to_en = translate(text, model_ru_to_en, tokenizer_ru_to_en, target_language)\n",
    "    # Переводим с английского обратно на русский\n",
    "    back_translated_to_ru = translate(translated_to_en, model_en_to_ru, tokenizer_en_to_ru, source_language)\n",
    "    return back_translated_to_ru\n",
    "\n",
    "# Кэширование переводов\n",
    "translation_cache = {}\n",
    "\n",
    "# Функция для аугментации с использованием back-translation\n",
    "def augment_text(text):\n",
    "    # Если текст уже переведен, используем кэшированный перевод\n",
    "    if text in translation_cache:\n",
    "        return translation_cache[text]\n",
    "    \n",
    "    # Иначе, выполняем back-translation\n",
    "    back_translated_text = back_translate(text)\n",
    "    translation_cache[text] = back_translated_text\n",
    "    return back_translated_text\n",
    "\n",
    "# Выборочная аугментация и многопоточность\n",
    "def selective_augmentation(texts, augmentation_probability=0.1, num_threads=10):\n",
    "    augmented_texts = []\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = []\n",
    "        for text in texts:\n",
    "            if random.random() < augmentation_probability:\n",
    "                futures.append(executor.submit(augment_text, text))\n",
    "            else:\n",
    "                augmented_texts.append(text)\n",
    "        \n",
    "        # Дождаться завершения всех операций перевода\n",
    "        for future in futures:\n",
    "            augmented_texts.append(future.result())\n",
    "    \n",
    "    return augmented_texts\n",
    "\n",
    "# Пример использования\n",
    "# texts_to_augment = [\"Пример текста для аугментации\", \"Еще один пример текста\"]\n",
    "# augmented_texts = selective_augmentation(texts_to_augment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# to gpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T05:35:34.754744Z",
     "start_time": "2023-12-05T05:35:34.708743100Z"
    }
   },
   "id": "95eea5ee185a82cd"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "'Example text for augmentation'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('Пример текста для аугментации', model_ru_to_en, tokenizer_ru_to_en, target_language=\"en\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T05:35:06.454518400Z",
     "start_time": "2023-12-05T05:35:03.870735900Z"
    }
   },
   "id": "ba9745a8ddebce71"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ff6db7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T05:34:04.396480100Z",
     "start_time": "2023-12-05T05:34:04.364950500Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m class_counts \u001B[38;5;241m=\u001B[39m \u001B[43mdata\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCLASS_ID\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalue_counts()\n\u001B[0;32m      2\u001B[0m minor_classes \u001B[38;5;241m=\u001B[39m class_counts[class_counts \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m10\u001B[39m]\u001B[38;5;241m.\u001B[39mindex\n",
      "\u001B[1;31mNameError\u001B[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "class_counts = data['CLASS_ID'].value_counts()\n",
    "minor_classes = class_counts[class_counts < 10].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac142815",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T05:34:08.083813800Z",
     "start_time": "2023-12-05T05:34:08.032489800Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'minor_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Создайте новый DataFrame с аугментированными данными\u001B[39;00m\n\u001B[0;32m      2\u001B[0m augmented_data \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m class_id \u001B[38;5;129;01min\u001B[39;00m \u001B[43mminor_classes\u001B[49m:\n\u001B[0;32m      4\u001B[0m     class_data \u001B[38;5;241m=\u001B[39m data[data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCLASS_ID\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m class_id]\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;66;03m# Собираем тексты для аугментации\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'minor_classes' is not defined"
     ]
    }
   ],
   "source": [
    "# Создайте новый DataFrame с аугментированными данными\n",
    "augmented_data = []\n",
    "for class_id in minor_classes:\n",
    "    class_data = data[data['CLASS_ID'] == class_id]\n",
    "\n",
    "    # Собираем тексты для аугментации\n",
    "    texts_to_augment = class_data['uu_usl_name'].tolist()\n",
    "\n",
    "    # Применяем selective_augmentation\n",
    "    augmented_texts = selective_augmentation(texts_to_augment)\n",
    "\n",
    "    # Добавляем аугментированные тексты в augmented_data\n",
    "    for augmented_text in augmented_texts:\n",
    "        augmented_data.append({'CLASS_ID': class_id, 'uu_usl_name': augmented_text})\n",
    "\n",
    "# Создайте новый DataFrame с аугментированными данными\n",
    "new_data = pd.DataFrame(augmented_data)\n",
    "\n",
    "# Объедините оригинальный DataFrame и новый DataFrame с аугментированными данными\n",
    "final_data = pd.concat([data, new_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c453b2e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-04T17:02:49.020214400Z"
    }
   },
   "outputs": [],
   "source": [
    "class_counts = final_data['CLASS_ID'].value_counts()\n",
    "minor_classes_count = class_counts.min()\n",
    "minor_classes_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4cbb1d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-04T17:02:49.028746Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ff81d1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-04T17:02:49.034265100Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^а-яА-Яa-zA-Z0-9]\", \" \", text)  # Удаляем все символы, кроме букв и цифр\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stop_words]  # Удаляем стоп-слова\n",
    "    text = \" \".join(words)\n",
    "    return text\n",
    "# Предполагается, что текстовый столбец называется 'text'\n",
    "final_data['uu_usl_name'] = final_data['uu_usl_name'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f8f0082894dcd30c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d15f0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-04T17:02:49.043796300Z"
    }
   },
   "outputs": [],
   "source": [
    "X = final_data['uu_usl_name']\n",
    "y = final_data[\"CLASS_ID\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_data['uu_usl_name'], final_data[\"CLASS_ID\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Препроцессинг\n",
    "X_train_preprocessed = [preprocess_text(text) for text in X_train]\n",
    "X_test_preprocessed = [preprocess_text(text) for text in X_test]\n",
    "\n",
    "# Векторизация\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_vectors = vectorizer.fit_transform(X_train_preprocessed)\n",
    "X_test_vectors = vectorizer.transform(X_test_preprocessed)\n",
    "\n",
    "# Создаем объект SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', k_neighbors=min(minor_classes_count - 1, 5), random_state=42)\n",
    "\n",
    "# Применяем SMOTE для балансировки классов\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_vectors, y_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485e3f33",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-04T17:02:49.047314Z"
    }
   },
   "outputs": [],
   "source": [
    "# Обучение модели\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "classifier.fit(X_resampled_vectors, y_resampled)\n",
    "y_pred = classifier.predict(vectorizer.transform(X_test))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# # Обучение модели\n",
    "# classifier = RandomForestClassifier(random_state=42, oob_score=True)\n",
    "# # Обучение модели\n",
    "# classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Оценка качества модели с использованием out-of-bag score\n",
    "# oob_score = classifier.oob_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b8f934",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-04T17:02:49.050321300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Предсказание и оценка\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.show()\n",
    "# y_pred = classifier.predict(vectorizer.transform(X_test))\n",
    "# print(f\"Out-of-Bag Score (оценка модели): {oob_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c435ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T17:02:49.071361300Z",
     "start_time": "2023-12-04T17:02:49.053322300Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b81c836",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-04T17:02:49.056843600Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
