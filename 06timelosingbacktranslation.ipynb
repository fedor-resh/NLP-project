{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "66b5be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, classification_report, roc_curve, auc, confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from transformers import BertTokenizer, BertForMaskedLM, pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import random\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1c205812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uu_usl_name</th>\n",
       "      <th>CLASS_ID</th>\n",
       "      <th>CNT_DIST_USL_BY_CLASS</th>\n",
       "      <th>CNT_NUM_USL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81601</th>\n",
       "      <td>Прием врачом-онкологом первичный амбулаторный</td>\n",
       "      <td>9000</td>\n",
       "      <td>169</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81602</th>\n",
       "      <td>Проводниковая анестезия</td>\n",
       "      <td>9000</td>\n",
       "      <td>169</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81603</th>\n",
       "      <td>Проводниковая анестезия (блокада по Оберсту-Лу...</td>\n",
       "      <td>9000</td>\n",
       "      <td>169</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81604</th>\n",
       "      <td>Проводниковая анестезия Sol. Ultracani D.S</td>\n",
       "      <td>9000</td>\n",
       "      <td>169</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81605</th>\n",
       "      <td>Удаление новообразования мягких тканей под мес...</td>\n",
       "      <td>9000</td>\n",
       "      <td>169</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             uu_usl_name  CLASS_ID  \\\n",
       "81601      Прием врачом-онкологом первичный амбулаторный      9000   \n",
       "81602                           Проводниковая анестезия       9000   \n",
       "81603  Проводниковая анестезия (блокада по Оберсту-Лу...      9000   \n",
       "81604         Проводниковая анестезия Sol. Ultracani D.S      9000   \n",
       "81605  Удаление новообразования мягких тканей под мес...      9000   \n",
       "\n",
       "       CNT_DIST_USL_BY_CLASS  CNT_NUM_USL  \n",
       "81601                    169            1  \n",
       "81602                    169            1  \n",
       "81603                    169            1  \n",
       "81604                    169            1  \n",
       "81605                    169            1  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./all_data.csv')\n",
    "data.drop_duplicates(subset='uu_usl_name', keep='first', inplace=True)\n",
    "data.drop('SERVICE_CLASS_CONFIRMED', axis=1, inplace=True)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5f00a6",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8e423d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Загрузка моделей для перевода\n",
    "model_name_ru_to_en = 'Helsinki-NLP/opus-mt-ru-en'\n",
    "model_name_en_to_ru = 'Helsinki-NLP/opus-mt-en-ru'\n",
    "\n",
    "tokenizer_ru_to_en = MarianTokenizer.from_pretrained(model_name_ru_to_en)\n",
    "model_ru_to_en = MarianMTModel.from_pretrained(model_name_ru_to_en)\n",
    "\n",
    "tokenizer_en_to_ru = MarianTokenizer.from_pretrained(model_name_en_to_ru)\n",
    "model_en_to_ru = MarianMTModel.from_pretrained(model_name_en_to_ru)\n",
    "\n",
    "# Функция для перевода текста\n",
    "def translate(text, model, tokenizer, target_language=\"en\"):\n",
    "    # Токенизируем текст\n",
    "    tokenized_text = tokenizer.prepare_seq2seq_batch([text], return_tensors='pt')\n",
    "    # Переводим текст\n",
    "    translated = model.generate(**tokenized_text)\n",
    "    # Декодируем переведенный текст\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Функция для back-translation\n",
    "def back_translate(text, source_language=\"ru\", target_language=\"en\"):\n",
    "    # Переводим с русского на английский\n",
    "    translated_to_en = translate(text, model_ru_to_en, tokenizer_ru_to_en, target_language)\n",
    "    # Переводим с английского обратно на русский\n",
    "    back_translated_to_ru = translate(translated_to_en, model_en_to_ru, tokenizer_en_to_ru, source_language)\n",
    "    return back_translated_to_ru\n",
    "\n",
    "# Кэширование переводов\n",
    "translation_cache = {}\n",
    "\n",
    "# Функция для аугментации с использованием back-translation\n",
    "def augment_text(text):\n",
    "    # Если текст уже переведен, используем кэшированный перевод\n",
    "    if text in translation_cache:\n",
    "        return translation_cache[text]\n",
    "    \n",
    "    # Иначе, выполняем back-translation\n",
    "    back_translated_text = back_translate(text)\n",
    "    translation_cache[text] = back_translated_text\n",
    "    return back_translated_text\n",
    "\n",
    "# Выборочная аугментация и многопоточность\n",
    "def selective_augmentation(texts, augmentation_probability=0.1, num_threads=10):\n",
    "    augmented_texts = []\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = []\n",
    "        for text in texts:\n",
    "            if random.random() < augmentation_probability:\n",
    "                futures.append(executor.submit(augment_text, text))\n",
    "            else:\n",
    "                augmented_texts.append(text)\n",
    "        \n",
    "        # Дождаться завершения всех операций перевода\n",
    "        for future in futures:\n",
    "            augmented_texts.append(future.result())\n",
    "    \n",
    "    return augmented_texts\n",
    "\n",
    "# Пример использования\n",
    "# texts_to_augment = [\"Пример текста для аугментации\", \"Еще один пример текста\"]\n",
    "# augmented_texts = selective_augmentation(texts_to_augment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd42e70",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3ff6db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = data['CLASS_ID'].value_counts()\n",
    "minor_classes = class_counts[class_counts < 10].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ac142815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/borisyakunin1311icloud.com/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3974: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m texts_to_augment \u001b[38;5;241m=\u001b[39m class_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muu_usl_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Применяем selective_augmentation\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m augmented_texts \u001b[38;5;241m=\u001b[39m selective_augmentation(texts_to_augment)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Добавляем аугментированные тексты в augmented_data\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m augmented_text \u001b[38;5;129;01min\u001b[39;00m augmented_texts:\n",
      "Cell \u001b[0;32mIn[51], line 61\u001b[0m, in \u001b[0;36mselective_augmentation\u001b[0;34m(texts, augmentation_probability, num_threads)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# Дождаться завершения всех операций перевода\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[0;32m---> 61\u001b[0m         augmented_texts\u001b[38;5;241m.\u001b[39mappend(future\u001b[38;5;241m.\u001b[39mresult())\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m augmented_texts\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Создайте новый DataFrame с аугментированными данными\n",
    "augmented_data = []\n",
    "for class_id in minor_classes:\n",
    "    class_data = data[data['CLASS_ID'] == class_id]\n",
    "\n",
    "    # Собираем тексты для аугментации\n",
    "    texts_to_augment = class_data['uu_usl_name'].tolist()\n",
    "\n",
    "    # Применяем selective_augmentation\n",
    "    augmented_texts = selective_augmentation(texts_to_augment)\n",
    "\n",
    "    # Добавляем аугментированные тексты в augmented_data\n",
    "    for augmented_text in augmented_texts:\n",
    "        augmented_data.append({'CLASS_ID': class_id, 'uu_usl_name': augmented_text})\n",
    "\n",
    "# Создайте новый DataFrame с аугментированными данными\n",
    "new_data = pd.DataFrame(augmented_data)\n",
    "\n",
    "# Объедините оригинальный DataFrame и новый DataFrame с аугментированными данными\n",
    "final_data = pd.concat([data, new_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c453b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts = final_data['CLASS_ID'].value_counts()\n",
    "minor_classes_count = class_counts.min()\n",
    "minor_classes_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4cbb1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "79ff81d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/borisyakunin1311icloud.com/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^а-яА-Яa-zA-Z0-9]\", \" \", text)  # Удаляем все символы, кроме букв и цифр\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stop_words]  # Удаляем стоп-слова\n",
    "    text = \" \".join(words)\n",
    "    return text\n",
    "# Предполагается, что текстовый столбец называется 'text'\n",
    "final_data['uu_usl_name'] = final_data['uu_usl_name'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "033d15f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected n_neighbors <= n_samples,  but n_samples = 3, n_neighbors = 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m smote \u001b[38;5;241m=\u001b[39m SMOTE(sampling_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, k_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(minor_classes_count \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m), random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Применяем SMOTE для балансировки классов\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m X_resampled, y_resampled \u001b[38;5;241m=\u001b[39m smote\u001b[38;5;241m.\u001b[39mfit_resample(X_train_vectors, y_train)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_resample(X, y)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/base.py:112\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    106\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[1;32m    110\u001b[0m )\n\u001b[0;32m--> 112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y)\n\u001b[1;32m    114\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    116\u001b[0m )\n\u001b[1;32m    118\u001b[0m X_, y_ \u001b[38;5;241m=\u001b[39m arrays_transformer\u001b[38;5;241m.\u001b[39mtransform(output[\u001b[38;5;241m0\u001b[39m], y_)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/over_sampling/_smote/base.py:364\u001b[0m, in \u001b[0;36mSMOTE._fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    361\u001b[0m X_class \u001b[38;5;241m=\u001b[39m _safe_indexing(X, target_class_indices)\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_k_\u001b[38;5;241m.\u001b[39mfit(X_class)\n\u001b[0;32m--> 364\u001b[0m nns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_k_\u001b[38;5;241m.\u001b[39mkneighbors(X_class, return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    365\u001b[0m X_new, y_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_samples(\n\u001b[1;32m    366\u001b[0m     X_class, y\u001b[38;5;241m.\u001b[39mdtype, class_sample, X_class, nns, n_samples, \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    367\u001b[0m )\n\u001b[1;32m    368\u001b[0m X_resampled\u001b[38;5;241m.\u001b[39mappend(X_new)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/neighbors/_base.py:808\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    806\u001b[0m n_samples_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_fit_\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_neighbors \u001b[38;5;241m>\u001b[39m n_samples_fit:\n\u001b[0;32m--> 808\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    809\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected n_neighbors <= n_samples, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    810\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but n_samples = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, n_neighbors = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_samples_fit, n_neighbors)\n\u001b[1;32m    811\u001b[0m     )\n\u001b[1;32m    813\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[1;32m    814\u001b[0m chunked_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected n_neighbors <= n_samples,  but n_samples = 3, n_neighbors = 6"
     ]
    }
   ],
   "source": [
    "X = final_data['uu_usl_name']\n",
    "y = final_data[\"CLASS_ID\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_data['uu_usl_name'], final_data[\"CLASS_ID\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Препроцессинг\n",
    "X_train_preprocessed = [preprocess_text(text) for text in X_train]\n",
    "X_test_preprocessed = [preprocess_text(text) for text in X_test]\n",
    "\n",
    "# Векторизация\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_vectors = vectorizer.fit_transform(X_train_preprocessed)\n",
    "X_test_vectors = vectorizer.transform(X_test_preprocessed)\n",
    "\n",
    "# Создаем объект SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', k_neighbors=min(minor_classes_count - 1, 5), random_state=42)\n",
    "\n",
    "# Применяем SMOTE для балансировки классов\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_vectors, y_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "485e3f33",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_resampled_vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Обучение модели\u001b[39;00m\n\u001b[1;32m      2\u001b[0m classifier \u001b[38;5;241m=\u001b[39m RandomForestClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m classifier\u001b[38;5;241m.\u001b[39mfit(X_resampled_vectors, y_resampled)\n\u001b[1;32m      4\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_test))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, y_pred))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_resampled_vectors' is not defined"
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "classifier.fit(X_resampled_vectors, y_resampled)\n",
    "y_pred = classifier.predict(vectorizer.transform(X_test))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# # Обучение модели\n",
    "# classifier = RandomForestClassifier(random_state=42, oob_score=True)\n",
    "# # Обучение модели\n",
    "# classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Оценка качества модели с использованием out-of-bag score\n",
    "# oob_score = classifier.oob_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b8f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказание и оценка\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.show()\n",
    "# y_pred = classifier.predict(vectorizer.transform(X_test))\n",
    "# print(f\"Out-of-Bag Score (оценка модели): {oob_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c435ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b81c836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
